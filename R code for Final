library(ggplot2) #for visualization
library(lubridate)
library(zoo)
library(dplyr) #data manipulation
library(scales) #map data to aes
library(tidyverse) #data manipulation
library(GGally) #helper of ggplot2
library(corrplot) #
library(MASS) #data functions
library(cowplot) #data visualization
library(caret) #functions for training and plotting regression models
library(car)
library(leaps)
library(moments)
RNGkind(sample.kind = "Rounding")
#****************************************************DATA SOURCE AND PREPROCESSING*******************************************************

#read in data and separate out the semicolons, make 1st row a header, omit any missing values. 
redwine_Data <- read.csv(file = "http://archive.ics.uci.edu/ml/machine-learning-databases/wine-quality/winequality-red.csv", header = TRUE, sep = ";") %>%
  na.omit()
attach(redwine_Data)
#shows dimensions of dataset
str(redwine_Data)

#summary of each column
summary(redwine_Data)


#*****************************************************EXPLORATORY DATA ANALYSIS*****************************************************

#making histograms/density plots to visualize spread of data for each predictor

hist1 <- ggplot(redwine_Data, aes(x=alcohol)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="Alcohol percentage", y="Density") + 
  geom_vline(aes(xintercept=mean(alcohol)), color="blue", linetype="dashed", size=1)

hist2 <- ggplot(redwine_Data, aes(x=pH)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="pH", y="Density") + 
  geom_vline(aes(xintercept=mean(pH)), color="blue", linetype="dashed", size=1)

hist3 <- ggplot(redwine_Data, aes(x=fixed.acidity)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="fixed acidity", y="Density") + 
  geom_vline(aes(xintercept=mean(fixed.acidity)), color="blue", linetype="dashed", size=1)

hist4 <- ggplot(redwine_Data, aes(x=volatile.acidity)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="volatile acidity", y="Density") + 
  geom_vline(aes(xintercept=mean(volatile.acidity)), color="blue", linetype="dashed", size=1)

hist5 <- ggplot(redwine_Data, aes(x=citric.acid)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="citric acid", y="Density") + 
  geom_vline(aes(xintercept=mean(citric.acid)), color="blue", linetype="dashed", size=1)

hist6 <- ggplot(redwine_Data, aes(x=residual.sugar)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="residual sugar", y="Density") + 
  geom_vline(aes(xintercept=mean(residual.sugar)), color="blue", linetype="dashed", size=1)

hist7 <- ggplot(redwine_Data, aes(x=chlorides)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="chlorides", y="Density") + 
  geom_vline(aes(xintercept=mean(chlorides)), color="blue", linetype="dashed", size=1)

hist8 <- ggplot(redwine_Data, aes(x=free.sulfur.dioxide)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="free sulfur dioxide", y="Density") + 
  geom_vline(aes(xintercept=mean(free.sulfur.dioxide)), color="blue", linetype="dashed", size=1)

hist9 <- ggplot(redwine_Data, aes(x=total.sulfur.dioxide)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="total sulfur dioxide", y="Density") + 
  geom_vline(aes(xintercept=mean(total.sulfur.dioxide)), color="blue", linetype="dashed", size=1)

hist10 <- ggplot(redwine_Data, aes(x=density)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="density", y="Density") + 
  geom_vline(aes(xintercept=mean(density)), color="blue", linetype="dashed", size=1)

hist11 <- ggplot(redwine_Data, aes(x=sulphates)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="sulphates", y="Density") + 
  geom_vline(aes(xintercept=mean(sulphates)), color="blue", linetype="dashed", size=1)

hist12 <- ggplot(redwine_Data, aes(x=quality)) + geom_histogram(aes(y=..density..),color="black", fill="white") + 
  geom_density(alpha=.2, fill="#FF6666") + labs(x="quality", y="Density") + 
  geom_vline(aes(xintercept=mean(quality)), color="blue", linetype="dashed", size=1)

plot_grid(hist1, hist2, hist3, hist4)
plot_grid(hist5, hist6, hist7, hist8)
plot_grid(hist9, hist10, hist11, hist12)
#density plots show that most distributions or positively skewed. pH and density are normally distributed. 
#----------------------------------------------------------------------------------------------------

#QQ plots
qqplot_pH <- ggplot(redwine_Data, aes(sample=pH)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="pH") + theme_cowplot()

qqplot_density <- ggplot(redwine_Data, aes(sample=density)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="density") + theme_cowplot()

qqplot_alcohol <- ggplot(redwine_Data, aes(sample=alcohol)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="alcohol") + theme_cowplot()

qqplot_chlorides <- ggplot(redwine_Data, aes(sample=chlorides)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="chlorides") + theme_cowplot()

qqplot_fixed.acidity <- ggplot(redwine_Data, aes(sample=fixed.acidity)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="fixed.acidity") + theme_cowplot()

qqplot_citric.acid <- ggplot(redwine_Data, aes(sample=citric.acid)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="citric.acid") + theme_cowplot()

qqplot_volatile.acidity <- ggplot(redwine_Data, aes(sample=volatile.acidity)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="volatile.acidity") + theme_cowplot()

qqplot_residual.sugar <- ggplot(redwine_Data, aes(sample=residual.sugar)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="residual.sugar") + theme_cowplot()

qqplot_free.sulfur.dioxide <- ggplot(redwine_Data, aes(sample=free.sulfur.dioxide)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="free.sulfur.dioxide") + theme_cowplot()

qqplot_total.sulfur.dioxide <- ggplot(redwine_Data, aes(sample=total.sulfur.dioxide)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="total.sulfur.dioxide") + theme_cowplot()

qqplot_sulphates <- ggplot(redwine_Data, aes(sample=sulphates)) + stat_qq() + stat_qq_line(color="steelblue", lwd=1) + 
  labs(x="Theoretical Quantiles", y="Sample Quantiles", title="sulphates") + theme_cowplot()

plot_grid(qqplot_pH, qqplot_density, qqplot_alcohol, qqplot_fixed.acidity)
plot_grid(qqplot_citric.acid, qqplot_chlorides, qqplot_volatile.acidity, qqplot_residual.sugar)
plot_grid(qqplot_free.sulfur.dioxide, qqplot_total.sulfur.dioxide, qqplot_sulphates)
#QQ plots of pH and density are shown to have near normal distribution. For the other variables, their data is close to normal
#distribution within 2 standard deviations of the mean but become right skewed. Extremely positive skewed data is not desirable
#since high levels can cause misleading data. For our case the data is not too positively skewed. 
#-----------------------------------------------------------------------------------------------------------


#Create boxplots of each predictor vs quality
box1 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=alcohol, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="alcohol", title="alcohol") + theme_classic() + theme(legend.position="none")

box2 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=sulphates, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="sulphates", title="sulphates") + theme_classic() + theme(legend.position="none") 

box3 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=residual.sugar, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="residual sugar", title="residual sugar") + theme_classic() + theme(legend.position="none")

box4 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=citric.acid, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="citric acid", title="citric acid") + theme_classic() + theme(legend.position="none") 

box5 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=chlorides, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="chlorides", title="chlorides") + theme_classic() + theme(legend.position="none")

box6 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=volatile.acidity, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="volatile acidity", title="volatile acidity") + theme_classic() + theme(legend.position="none") 

box7 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=fixed.acidity, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="fixed acidity", title="fixed acidity") + theme_classic() + theme(legend.position="none") 

box8 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=pH, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="pH", title="pH") + theme_classic() + theme(legend.position="none")

box9 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=density, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="density", title="density") + theme_classic() + theme(legend.position="none")

box10 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=total.sulfur.dioxide, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="total sulfur dioxide", title="total SO2") + theme_classic() + theme(legend.position="none")

box11 <- ggplot(redwine_Data, aes(x=as.factor(quality), y=free.sulfur.dioxide, color=as.factor(quality))) + geom_boxplot() + 
  labs(x="quality", y="free sulfur dioxide", title="free SO2") + theme_classic() + theme(legend.position="none")

plot_grid(box1,box2,box3,box4)
plot_grid(box5,box6,box7,box8)
plot_grid(box9,box10,box11)
#look for pattern in these boxplots
#We see linear positive relationship with quality from alchohol, sulphates, fixed acidity, and citric acid.
#We also see a linear negative relationship with quality from density, volatile acidity, pH and chlorides 

#display table of quality scores. Most the scores are in 5 and 6 with little data for lower and higher scores. 
table(redwine_Data$quality)


#-----------------------------------Feature engineering-------------------------------------------------
#Remove outliers, I will add code upon this. 
summary(redwine_Data)
#Looking at the summary of each variable, we intially see there are outliers in most of the variables where the
#max value and min values are very far from the mean of each column. 
#We will be using Cook's Distance to remove these outliers as it is an estimate of a data point's influence.
#Cook's distance takes into account an obeservation's leverage and residual. We will investigate points that
#are more than 4x the mean of all distances. 

#First use a multiple linear regression model as a baseline model to compare
model1 <- lm(quality~., data=redwine_Data)
summary(model1)
#baseline model has adjusted R-squared of 0.3561
par(mfrow=c(2,2))
plot(model1)
#----------------------------------------------------------------------------------------------------

#From the diagnostic plots, we can see some outliers which we can remove to make our model a better fit. We use
#cooks.distance function on our model to filter out values greater than 4x the mean. We plot the output of 
#cooks.distance function to visualize which points are 4x the mean.
par(mfrow=c(1,1))
cooksD <- cooks.distance(model1)
plot(cooksD, pch = "x", cex=1.5, main="Cooks distance influential points")
abline(h=4*mean(cooksD, na.rm=TRUE), col = "blue", lwd = 2)

par(mfrow=c(3,2))
plot(lm(quality~alcohol, data=redwine_Data), which=c(5), main="alcohol")
plot(lm(quality~citric.acid, data=redwine_Data), which=c(5), main="ctric.acid")
plot(lm(quality~total.sulfur.dioxide, data=redwine_Data), which=c(5), main="total.sulfur.dioxide")
plot(lm(quality~pH, data=redwine_Data), which=c(5), main="pH")
plot(lm(quality~chlorides, data=redwine_Data), which=c(5), main="chlorides")
plot(lm(quality~fixed.acidity, data=redwine_Data), which=c(5), main="fixed.acidity")
#The figure above shows some examples of the outliers having significant leverage. 

influential_points <- cooksD[(cooksD > (4*mean(cooksD, na.rm=TRUE)))]
wine_outliers <- redwine_Data[names(influential_points),]
redwine_Data2 <- anti_join(redwine_Data, wine_outliers)
#we see 67 data outliers were removed in new dataset. 

model2 <- lm(redwine_Data2$quality~., data=redwine_Data2)
summary(model2)
#The adjusted R-squared has improved from 0.3561 to 0.3989(a 12% increase). The P-values have decreased for the majority of the 
#features as well. 

par(mfrow=c(2,2))
plot(model2)
#The new diagnostic plots have improved as well. The qqplot shows better normal distribution. The Residuals vs Leverage
#plot does not have any outstanding points with great leverage. Shows we can use linear regression with 4 assumptions.

#Linear regression is an analysis that assesses whether one or more predictor variables explain the dependent (criterion) variable.
#The regression has five key assumptions:
#Linear relationship
#Multivariate normality
#No or little multicollinearity
#No auto-correlation
#Homoscedasticity


skew1 <- skewness(redwine_Data$fixed.acidity)
skew1 <- append(skew1, skewness(redwine_Data$volatile.acidity))
skew1 <- append(skew1, skewness(redwine_Data$citric.acid))
skew1 <- append(skew1, skewness(redwine_Data$residual.sugar))
skew1 <- append(skew1, skewness(redwine_Data$chlorides))
skew1 <- append(skew1, skewness(redwine_Data$free.sulfur.dioxide))
skew1 <- append(skew1, skewness(redwine_Data$total.sulfur.dioxide))
skew1 <- append(skew1, skewness(redwine_Data$density))
skew1 <- append(skew1, skewness(redwine_Data$pH))
skew1 <- append(skew1, skewness(redwine_Data$sulphates))
skew1 <- append(skew1, skewness(redwine_Data$alcohol))
skew1 <- append(skew1, skewness(redwine_Data$quality))

skew2 <- skewness(redwine_Data2$fixed.acidity)
skew2 <- append(skew2, skewness(redwine_Data2$volatile.acidity))
skew2 <- append(skew2, skewness(redwine_Data2$citric.acid))
skew2 <- append(skew2, skewness(redwine_Data2$residual.sugar))
skew2 <- append(skew2, skewness(redwine_Data2$chlorides))
skew2 <- append(skew2, skewness(redwine_Data2$free.sulfur.dioxide))
skew2 <- append(skew2, skewness(redwine_Data2$total.sulfur.dioxide))
skew2 <- append(skew2, skewness(redwine_Data2$density))
skew2 <- append(skew2, skewness(redwine_Data2$pH))
skew2 <- append(skew2, skewness(redwine_Data2$sulphates))
skew2 <- append(skew2, skewness(redwine_Data2$alcohol))
skew2 <- append(skew2, skewness(redwine_Data2$quality))

summary_skew <- rbind(skew1, skew2)
colnames(summary_skew) <- c("fixed.acidity", "volatile.acidity", "citric.acid", "residual.sugar", "chlorides",
                            "free.sulfur.dioxide", "total.sulfur.dioxide", "density", "pH", 
                            "sulphates", "alcohol","quality")
rownames(summary_skew) <- c("Before", "After")
summary_skew
#The skewness after the outliers were removed has decreased for most variables. Skewness values between -0.5
#and 0.5 mean the distribution is approximately symmetric. Residual.sugar and chlorides are still highly skewed 
#to the right. 

#----------------------------------------------------------------------------------------------------

# we could log transform the data to help fix the positively skewed data for several variables. When we look at a plot of the
# log transformation, it does not improve the normalization or the linearity between quality and the predictors. Will most likely 
#not need to transform

#log_redwine <- log(redwine_Data[,1:11])
#log_redwine$quality <- redwine_Data$quality


#********************************************FEATURE SELECTION*********************************************

#find correlations, use cor() to produce matrix and corrplot to display a plot 
par(mfrow=c(1,1))
cor(redwine_Data2)
corrplot(cor(redwine_Data2), method="shade", type="lower",addCoef.col = "black",diag=FALSE,number.cex=0.7)
#density and fixed.acidity (0.68), fixed.acidity and citric acid (0.68), fixed.acidity and pH (-0.69), free.sulfur.dioxide
#and total.sulfur.dioxide (0.68) have strong correlation, perhaps high multicollinearity

#display in order the value the predictors correlate to Quality 
rankQuality <- cor(redwine_Data2)[-12,12] %>% abs()
head(rankQuality[order(rankQuality, decreasing=TRUE)],12)
#We see that alcohol, volatile.acidity, sulphates, and citric acid have the highest correlation with quality.

#----------------------------------------------------------------------------------------------------
#split dataset into training and testing, 70:30 split. Leaves 1119 obs for training and 381 obs for testing.

set.seed(3)
redwine_training <- sample_frac(tbl=redwine_Data2, replace = FALSE, size = 0.80)
redwine_test <- anti_join(redwine_Data2, redwine_training)


#We also want to check multicollinearity(variables in a regression are correlated with each other). If we have
#multicollinearity then we will not know exactly which variables are truly predictive of the outcome.
#Thus we use the variance inflation factor(VIF) function to compute a multicollinearity score. The VIF measures
#by how much the variance of the coefficient is inflated from multicollinearity.
library(car)
model2 <- lm(quality~., data=redwine_training)
vif(model2)
#All VIF values are below the generally accepted value of 10. Fixed acidity and density values show
#that they are highly correlated We'll try a model without fixed.acidity
model3 <- lm(quality~.-fixed.acidity, data=redwine_training)
vif(model3)
#Without fixed.acidity, VIF values of other variables have decreased, especially density. We can consider 
#removing density as well.
#----------------------------------------------------------------------------------------------------

#choosing model size using k-fold cross validation
library(leaps)
#write predict method to use regsubsets() since there is no predict() method for regsubsets()
predict.regsubsets =function (object , newdata ,id ,...){
  form=as.formula (object$call [[2]])
  mat=model.matrix(form ,newdata )
  coefi=coef(object ,id=id)
  xvars=names(coefi)
  mat[,xvars]%*%coefi
}


reg.best=regsubsets(quality~.,data=redwine_Data2 , nvmax=11)
k=10
set.seed(3)
folds=sample (1:k,nrow(redwine_Data2),replace=TRUE)
cv.errors=matrix (NA,k,11, dimnames =list(NULL , paste (1:11) ))

#ISLR for loop using predict method above
for(j in 1:k){
  best.fit=regsubsets(quality~.,data=redwine_Data2[folds!=j,], nvmax=11)
  for(i in 1:11){
     pred=predict(best.fit ,redwine_Data2[folds ==j,],id=i)
     cv.errors[j,i]= mean( ( redwine_Data2$quality[ folds==j]-pred)^2)
    }
}
mean.cv.errors=apply(cv.errors ,2, mean)
mean.cv.errors
par(mfrow=c(1,1))
plot(mean.cv.errors ,type="b")

coef(reg.best ,7)
#Based on plot, we will use 7 variables instead of 11. They both have near identical values but 7 variables is simpler than 11. 
#These are the variables we will be using for all models. These variables chosen are in argeement with what we found in our
#correlation ranking and VIF findings. 
#1. volatile.acidity
#2. chlorides
#3. free.sulfur.dioxide
#4. total.sulfur.dioxide
#5. pH
#6. sulphates
#7. alcohol


#***************************************************MODELING******************************************************************
#Need RMSE, R2, MAE, MSE
library(modelr)
#Multiple Linear Regression
set.seed(3)
multi_winefit <- lm(quality~volatile.acidity + chlorides + free.sulfur.dioxide + total.sulfur.dioxide + pH + sulphates + alcohol, 
                    data=redwine_training)

summary(multi_winefit)
#Combining the results of or EDA, correlation, and feature selection we have chosen to prioritize seven variables out of 
#We used the 7 variables chosen by stepwise k-fold cross validation regression for the model of multiple linear regression. 
#Multiple linear regression is a simple and good basis model to compare our other models to predict quality. The chosen variables
#are highly correlated to our dependent quality variable and their p-value shows their statistical significance. 

#Performance using train dataset
multi_R2_training <- rsquare(multi_winefit, redwine_training) #R-squared 0.391984
multi_RMSE_training <- rmse(multi_winefit, redwine_training) #Root Mean Squared Error 0.581757
multi_MAE_training <- mae(multi_winefit, redwine_training) #Mean Absolute Error 0.462793


#Performance using test dataset
multi_R2_test <- rsquare(multi_winefit, redwine_test) #R-squared 0.491427
multi_RMSE_test <- rmse(multi_winefit, redwine_test) #Root Mean Squared Error 0.539477
multi_MAE_test <- mae(multi_winefit, redwine_test) #Mean Absolute Error 0.434611

#Make a confusion matrix
multi_pred_test <- predict(multi_winefit, redwine_test) #predict with test dataset
multi_pred_round <- round(multi_pred_test) #round off values to whole numbers for confusion matrix
multi_confusion_matrix <- table(predicted = multi_pred_round, actual = redwine_test$quality)
multi_confusion_matrix
#accuracy of the model on test data
mean(multi_pred_round==redwine_test$quality) #accuracy is 65%

#----------------------------------------------------------------------------------------------------
#LASSO
library(glmnet)

#prepare model matrix training and test sets
xtrain <- model.matrix(quality~., redwine_training)[,-1]
ytrain <- redwine_training$quality
xtest <- model.matrix(quality~., redwine_test)[,-1]
ytest <- redwine_test$quality

set.seed(3)
cv.lasso.wine <- cv.glmnet(xtrain, ytrain, alpha=1)
plot(cv.lasso.wine)
#obtain best lambda for least MSE
bestlam <- cv.lasso.wine$lambda.min # 0.0006777187

#model with best lambda
lasso.best.wine <- glmnet(xtrain, ytrain, alpha=1, lambda=bestlam)
coef(lasso.best.wine)

#Performance metrics on train dataset
lasso.pred2 <- predict(lasso.best.wine, xtrain)
lasso_R2_winetrain <- cor(ytest, lasso.pred2)^2 #0.0.3953817
lasso_MAE_winetrain <- mean(abs(ytest-lasso.pred2)) #0.4617966
lasso_RMSE_winetrain <- sqrt(mean((ytest-lasso.pred2)^2)) #0.0.5801316

#Performance metrics on test dataset
lasso.pred <- predict(lasso.best.wine, xtest)
lasso_R2_wine <- cor(ytest, lasso.pred)^2 #0.492290
lasso_MAE_wine <- mean(abs(ytest-lasso.pred)) #0.4366197
lasso_RMSE_wine <- sqrt(mean((ytest-lasso.pred)^2)) #0.5404831

#Make a confusion matrix
lasso_pred_round <- round(lasso.pred) #round off values to whole numbers for confusion matrix
lasso_confusion_matrix <- table(predicted = lasso_pred_round, actual = redwine_test$quality)
lasso_confusion_matrix
#accuracy of the model on test data
mean(lasso_pred_round==redwine_test$quality) #accuracy is 63%
#----------------------------------------------------------------------------------------------------
#Partial Least Squares
library(pls)

set.seed(3)
pls.redwine <- plsr(quality~., data=redwine_training, scale=TRUE, validation="CV")
summary(pls.redwine)
validationplot(pls.redwine, val.type="MSEP")
# we see through the summary and plot that lowest cross-validation error occurs when M=7(ncomp=7) partial
#least squares directions are used. 

#Performace using train dataset
pls.R2.training <- rsquare(pls.redwine, redwine_training) #0.389671
pls.RMSE.training <- rmse(pls.redwine, redwine_training) #0.583079
pls.MAE.training <- mae(pls.redwine, redwine_training) #0.464885

#Performace using test dataset
pls.R2.test <- rsquare(pls.redwine, redwine_test) #0.491523
pls.RMSE.test <- rmse(pls.redwine, redwine_test) #0.541014
pls.MAE.test <- mae(pls.redwine, redwine_test) #0.435541

pls_pred_test <- predict(pls.redwine, newdata=redwine_test, ncomp=7)
pls_pred_round <- round(pls_pred_test)
pls_confusion_matrix <- table(fitted.values = pls_pred_round, actual = redwine_test$quality)
pls_confusion_matrix
#accuracy of the model on test data
mean(pls_pred_round==redwine_test$quality) #Accuracy is 63%


#----------------------------------------------------------------------------------------------------

#RandomForest
library(randomForest)

set.seed(3)
#rf.redwine <- randomForest(quality~volatile.acidity + chlorides + total.sulfur.dioxide + 
#                           pH + sulphates + alcohol, data=redwine_training, mtry = 2, importance=TRUE)
rf.redwine <- randomForest(quality~., data=redwine_training, mtry = 11/3, importance=TRUE)
#summary of model
print(rf.redwine)

#variable importance plot,The former is based upon the mean decrease of accuracy in predictions on the
#out of bag samples when a given variable is excluded from the model. The latter is a measure
#of the total decrease in node impurity that results from splits over that variable, averaged over all trees
par(mfrow=c(1,1))
varImpPlot(rf.redwine, type=1, main="Accuracy Decrease")

#Evaluation
#Performance using train dataset
rf_R2_training <- rsquare(rf.redwine, redwine_training) #R-squared 0.509190
rf_RMSE_training <- rmse(rf.redwine, redwine_training) #Root Mean Squared Error 0.523912
rf_MAE_training <- mae(rf.redwine, redwine_training) #Mean Absolute Error 0.394347

#Performance using test dataset

rf_R2_test <- rsquare(rf.redwine, redwine_test) #R-squared 0.525642
rf_RMSE_test <- rmse(rf.redwine, redwine_test) #Root Mean Squared Error 0.523476
rf_MAE_test <- mae(rf.redwine, redwine_test) #Mean Absolute Error 0.415944

#Make a confusion matrix
rf_pred_test <- predict(rf.redwine, newdata=redwine_test) #predict with test dataset
rf_pred_round <- round(rf_pred_test) #round off values to whole numbers for confusion matrix
rf_confusion_matrix <- table(predicted = rf_pred_round, actual = redwine_test$quality)
rf_confusion_matrix
#accuracy of the model on test data
mean(rf_pred_round==redwine_test$quality) #accuracy is 67%

#----------------------------------------------------------------------------------------------------




# 
#boosting
# library(gbm)
# set.seed(3)
# boost.wine <- gbm(quality~volatile.acidity + chlorides + free.sulfur.dioxide + total.sulfur.dioxide +
#                 pH + sulphates + alcohol, data=redwine_training, distribution="gaussian",
#                  n.trees=500, interaction.depth=4, cv.folds=5, shrinkage=0.2, verbose = F)
# prefcv <- gbm.perf(boost.wine)
# prefcv
# summary(boost.wine)
# red.boost <- predict(boost.wine, newdata=redwine_test, n.trees=prefcv)
# boost_R2_test <- R2(red.boost, redwine_test$quality)
# boost_RMSE_test <- RMSE(red.boost, redwine_test$quality)
# boost_MAE_test <- MAE(red.boost, redwine_test$quality)
# 
# boost_MAE_test
# boost_R2_test
# boost_RMSE_test
# 
# red.boost.round <- round(red.boost)
# boost_confusion_matrix <- table(predicted = red.boost.round, actual = redwine_test$quality)
# boost_confusion_matrix
# mean(red.boost.round==redwine_test$quality)
